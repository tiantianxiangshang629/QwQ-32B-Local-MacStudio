# QwQ-32B-Local-MacStudio [Deploy in 20mins]

Deploy QwQ-32B locally on Mac Studio (M3 Ultra) using MLX + ModelScope.  
Experience fast, private inference of a 32B-parameter LLM optimized for Apple Silicon.  
Powered by [ModelScope](https://modelscope.cn) + [MLX](https://github.com/ml-explore/mlx).

ğŸš€ No cloud. No latency. 100% local inference.

åœ¨ Mac Studioï¼ˆM3 Ultraï¼‰ä¸Šä½¿ç”¨ MLX å’Œ ModelScope æœ¬åœ°éƒ¨ç½² QwQ-32B æ¨¡å‹ã€‚
ä½“éªŒä¸º Apple Silicon ä¼˜åŒ–çš„ 320 äº¿å‚æ•°å¤§è¯­è¨€æ¨¡å‹çš„é«˜é€Ÿã€ç§å¯†æœ¬åœ°æ¨ç†ã€‚
ç”± ModelScope å’Œ MLX æä¾›æŠ€æœ¯æ”¯æŒã€‚

ğŸš€ æ— éœ€äº‘ç«¯ï¼Œæ— å»¶è¿Ÿï¼Œ100% æœ¬åœ°æ¨ç†ã€‚
çŸ¥ä¹ä¸“æ ï¼š[åœ¨ Mac ä¸Šæœ¬åœ°éƒ¨ç½² QwQ-32B æ¨¡](https://zhuanlan.zhihu.com/p/1889893175897872136)

---

## âœ¨ Demo

![QwQ-32B Gradio Demo](demo.gif)

---

## ğŸ§  Features

- ğŸ¤– Open-source chatbot interface powered by Gradio  
- ğŸ§± Quantized 8-bit QwQ-32B model optimized for Mac  
- âš¡ï¸ MLX backend tailored for Apple Silicon (M3, M2, M1)  
- ğŸ”§ Easy, Conda-based installation  
- ğŸª„ No GPU or cloud dependency

---

## ğŸ§° System Requirements

- Mac Studio (M3 Ultra recommended)  
- macOS 15+ (tested on Sequoia 15.3)  
- 16GB+ RAM (96GB recommended)  
- Python 3.12  
- ~30GB disk space for model files

---

## ğŸ› ï¸ Installation

### Step 1: Install Miniconda

```bash
mkdir -p ~/miniconda3
curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh -o ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
source ~/miniconda3/bin/activate
conda init --all
```

### Step 2: Create a new Conda environment

```bash
conda create -n qwq python=3.12 -y
conda activate qwq
```

### Step 3: Install dependencies

```bash
pip install modelscope
pip install mlx==0.21.1
pip install mlx-lm==0.21.5
pip install mlx-vlm==0.1.12
pip install gradio
```

### Step 4: Run the Chatbot

```bash
python app.py
```

Then open your browser at: [http://127.0.0.1:7860](http://127.0.0.1:7860)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ¤ Credits

- [ModelScope](https://modelscope.cn/)
- [MLX](https://github.com/ml-explore/mlx)
- [QwQ-32B by @okwinds](https://modelscope.cn/models/okwinds/QwQ-32B-Preview-MLX-8bit)
